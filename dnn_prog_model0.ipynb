{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for optimal DNN architecute\n",
    "This notebook is designed only to search the optimal DNN architecture for the coupled-channel pole classification problem.\n",
    "We use the standard `Classifier` of `Chainer` to wrap each architecture with the default hyperparameters. The DNN models with fixed number of hidden layers are imported from `dnn_models` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from dnn_models.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import chainer\n",
    "from chainer import configuration\n",
    "from chainer.dataset import convert\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "from chainer import optimizers, initializers, serializers\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import random\n",
    "\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "import pickle\n",
    "\n",
    "import import_ipynb\n",
    "import dnn_models\n",
    "from dnn_models import MLP1L, MLP2L, MLP3L\n",
    "from dnn_models import MLP4L, MLP5L, MLP6L\n",
    "from dnn_models import MLP4LDRP, MLP5LDRP\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose curricululum to train\n",
    "curriculum = 32\n",
    "\n",
    "resume0 = True\n",
    "#General resume\n",
    "\n",
    "resume1 = True\n",
    "#Set resume1 to False if you want to continue using main directory\n",
    "#Set resume1 to True if you want to resume using snapshot directory\n",
    "\n",
    "resume2 = False\n",
    "#Set resume2 to True if you want to continue using PREVIOUS CURRICULUM snapshot directory\n",
    "#Set resume2 to False otherwise\n",
    "\n",
    "#Which last epoch would you like to continue?\n",
    "last_epoch = 31090 #31070 #31050\n",
    "#Set minibatch size\n",
    "batchsize = 3*512 #2*512 #512\n",
    "\n",
    "#For resume=False. If you want to initialize weights (I am not sure if this is truly effective)\n",
    "initialize = True\n",
    "\n",
    "#Set maximum epoch of the full training\n",
    "max_epoch = 100000 - last_epoch\n",
    "#Set maximum repetitions during epoch restart\n",
    "max_rep = 10\n",
    "#Set accuracy drop tolerance to execute epoch restart\n",
    "drop_tolerance = -0.005\n",
    "#Save model and state every save_epoch\n",
    "save_epoch = 10\n",
    "\n",
    "#Choose your DNN model\n",
    "dnn1 = MLP3L(200,200,200)\n",
    "\n",
    "#local directory and file name of training and testing curriculum dataset\n",
    "#trainset = 'curriculum_trainset//chainer_train_curr{:02d}.pkl'.format(curriculum)\n",
    "#testset = 'curriculum_testset//chainer_test_curr{:02d}.pkl'.format(curriculum)\n",
    "trainset = 'chainer_train_curr{:02d}.pkl'.format(curriculum)\n",
    "testset = 'chainer_test_curr{:02d}.pkl'.format(curriculum)\n",
    "\n",
    "#Continuous training directory\n",
    "directory1 = 'dnn_curr{:02d}_full'.format(curriculum)\n",
    "#Snapshot directory (save models every save_epoch)\n",
    "directory2 = 'dnn_curr{:02d}_snapshot'.format(curriculum)\n",
    "\n",
    "gpu_id = 1\n",
    "\n",
    "#Load previous training results from the snapshot folder\n",
    "prev_curr = curriculum\n",
    "if curriculum > 1:\n",
    "    prev_curr = curriculum - 1\n",
    "\n",
    "out = directory1\n",
    "if not os.path.isdir(out):\n",
    "    os.makedirs(out)\n",
    "\n",
    "out2 = directory2\n",
    "if not os.path.isdir(out2):\n",
    "    os.makedirs(out2)\n",
    "\n",
    "#present curriculum directory\n",
    "if resume1 == True and resume2 == False:\n",
    "    directory3 = directory2\n",
    "#previous curriculum directory\n",
    "if resume1 == True and resume2 == True:\n",
    "    directory3 = 'dnn_curr{:02d}_snapshot'.format(prev_curr)\n",
    "\n",
    "    \n",
    "if resume1 == True:        \n",
    "    import shutil\n",
    "    shutil.copy(os.path.join(directory3,'epoch{:06d}//MLP1.model'.format(last_epoch)),directory1)\n",
    "    shutil.copy(os.path.join(directory3,'epoch{:06d}//MLP1.state'.format(last_epoch)),directory1)\n",
    "    shutil.copy(os.path.join(directory3,'epoch{:06d}//testing_accu1.pkl'.format(last_epoch)),directory1)\n",
    "    shutil.copy(os.path.join(directory3,'epoch{:06d}//testing_loss1.pkl'.format(last_epoch)),directory1)\n",
    "    shutil.copy(os.path.join(directory3,'epoch{:06d}//training_accu1.pkl'.format(last_epoch)),directory1)\n",
    "    shutil.copy(os.path.join(directory3,'epoch{:06d}//training_loss1.pkl'.format(last_epoch)),directory1)\n",
    "    shutil.copy(os.path.join(directory3,'epoch{:06d}//epoch_log.txt'.format(last_epoch)),directory1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_dnn():\n",
    "    #GPU usage is only activated here.\n",
    "    \n",
    "    device = chainer.get_device(gpu_id)\n",
    "    model1 = L.Classifier(dnn1)\n",
    "    model1.to_device(device)\n",
    "    device.use()\n",
    "    \n",
    "    optimizer1 = chainer.optimizers.Adam()\n",
    "    optimizer1 = optimizer1.setup(model1)\n",
    "    \n",
    "    # Weight initilizer\n",
    "    if resume0 == False and resume1 == False and initialize == True:\n",
    "        chainer.initializers.HeNormal(model1)\n",
    "    \n",
    "    #Load training and testing datasets\n",
    "    train = pickle.load(open(trainset,'rb'))\n",
    "    test = pickle.load(open(testset,'rb'))\n",
    "    \n",
    "    #Define Iterator\n",
    "    train_iter = chainer.iterators.SerialIterator(train, batchsize)\n",
    "    test_iter = chainer.iterators.SerialIterator(test, batchsize, repeat=False, shuffle=False)\n",
    "    \n",
    "    out = directory1\n",
    "    if not os.path.isdir(out):\n",
    "        os.makedirs(out)\n",
    "\n",
    "    out2 = directory2\n",
    "    if not os.path.isdir(out2):\n",
    "        os.makedirs(out2)        \n",
    "        \n",
    "    if resume0 == True:\n",
    "        #Load log book\n",
    "        log = open(os.path.join(out,'epoch_log.txt'),'a+')\n",
    "        #Load lists of training and testing accuracies of the previous run \n",
    "        training_accu1 = pickle.load(open(os.path.join(out,\"training_accu1.pkl\"),\"rb\"))\n",
    "        testing_accu1 = pickle.load(open(os.path.join(out,\"testing_accu1.pkl\"),\"rb\"))\n",
    "        #Load lists of traininf and testing losses of the previous run \n",
    "        training_loss1 = pickle.load(open(os.path.join(out,\"training_loss1.pkl\"),\"rb\"))\n",
    "        testing_loss1 = pickle.load(open(os.path.join(out,\"testing_loss1.pkl\"),\"rb\"))\n",
    "        #Load the model and the optimizer of the previous run\n",
    "        serializers.load_npz(os.path.join(out,\"MLP1.model\"), model1)\n",
    "        serializers.load_npz(os.path.join(out,\"MLP1.state\"), optimizer1)\n",
    "                                     \n",
    "        #I need this to assign value to last epoch. Batch size might change and we cannot rely on the iterators\n",
    "        training_accu1 = pickle.load(open(os.path.join(out,\"training_accu1.pkl\"),\"rb\"))\n",
    "        \n",
    "    elif resume0 == False:\n",
    "        log = open(os.path.join(out,'epoch_log.txt'),'w+')\n",
    "        log = open(os.path.join(out,'epoch_log.txt'),'a+')\n",
    "        #Initialize lists of losses and accuracies\n",
    "        training_loss1 = []\n",
    "        training_accu1 = []\n",
    "        testing_loss1 = []\n",
    "        testing_accu1 = []\n",
    "        \n",
    "        #I need this to assign value to last epoch\n",
    "        training_accu1 = [] \n",
    "    \n",
    "    last_epoch = len(training_accu1)\n",
    "        \n",
    "    \n",
    "    time_start = time.time()    \n",
    "    test_count = len(test)\n",
    "    \n",
    "    #Initialize training\n",
    "    train_count = 0\n",
    "    \n",
    "    sum1_loss = 0\n",
    "    sum1_accu = 0\n",
    "    \n",
    "    rep = 0\n",
    "    #---------------------------start training epoch----------------------------------------------\n",
    "    while (train_iter.epoch < max_epoch) and (rep < max_rep):\n",
    "        batch = train_iter.next()\n",
    "        x, t = convert.concat_examples(batch, device)\n",
    "        train_count += len(t)\n",
    "        #Update network's parameters using forward pass and backpropagation for each model\n",
    "        optimizer1.update(model1, x, t)\n",
    "        #Calculate training loss and accuracy\n",
    "        sum1_loss += float(model1.loss.array)*len(t)\n",
    "        sum1_accu += float(model1.accuracy.array)*len(t)\n",
    "    #-----------------------end of one epoch----------------------------------------------------\n",
    "        \n",
    "        if train_iter.is_new_epoch:\n",
    "            #Record training loss and accuracy for each model\n",
    "            training_loss1.append(sum1_loss/train_count)\n",
    "            training_accu1.append(sum1_accu/train_count)\n",
    "            #Initialize loss and accuracy for testing\n",
    "            sum1_loss = 0\n",
    "            sum1_accu = 0\n",
    "            \n",
    "            #Enable evaluation mode\n",
    "            with configuration.using_config('train', False):\n",
    "                #This is optional but can reduce computational overhead\n",
    "                with chainer.using_config('enable_backprop', False):\n",
    "                    for batch in test_iter:\n",
    "                        x, t = convert.concat_examples(batch, device)\n",
    "                        #Calculate testing loss and accuracy\n",
    "                        loss1 = model1(x,t)\n",
    "                        sum1_loss += float(loss1.array)*len(t)\n",
    "                        sum1_accu += (model1.accuracy.array)*len(t)\n",
    "                        \n",
    "            test_iter.reset()\n",
    "            \n",
    "            #I need this to assign value to the recent epoch\n",
    "            epoch = len(training_accu1)            \n",
    "            \n",
    "            #Record testing loss and accuracy\n",
    "            testing_loss1.append(sum1_loss/test_count)\n",
    "            testing_accu1.append(sum1_accu/test_count)\n",
    "            \n",
    "            #Restart if training is too bad\n",
    "            drop = float(training_accu1[len(training_accu1)-1]) - float(training_accu1[len(training_accu1)-2])\n",
    "            retrain = 0\n",
    "            if drop < drop_tolerance:\n",
    "                retrain = retrain + 1\n",
    "                rep = retrain\n",
    "                #Load log book\n",
    "                log = open(os.path.join(out,'epoch_log.txt'),'a+')\n",
    "                log.write('We are restarting at epoch {:06d} for {:03d} times \\r\\n'.format(epoch, retrain))\n",
    "                #Load lists of training and testing accuracies of the previous run \n",
    "                training_accu1 = pickle.load(open(os.path.join(out,\"training_accu1.pkl\"),\"rb\"))\n",
    "                testing_accu1 = pickle.load(open(os.path.join(out,\"testing_accu1.pkl\"),\"rb\"))\n",
    "                #Load lists of training and testing losses of the previous run \n",
    "                training_loss1 = pickle.load(open(os.path.join(out,\"training_loss1.pkl\"),\"rb\"))\n",
    "                testing_loss1 = pickle.load(open(os.path.join(out,\"testing_loss1.pkl\"),\"rb\"))\n",
    "                #Load the model and the optimizer of the previous run\n",
    "                serializers.load_npz(os.path.join(out,\"MLP1.model\"), model1)\n",
    "                serializers.load_npz(os.path.join(out,\"MLP1.state\"), optimizer1)\n",
    "                #I need this to assign value to the recent epoch\n",
    "                epoch = len(training_accu1)\n",
    "                \n",
    "            \n",
    "            #Save model and optimizer state\n",
    "            serializers.save_npz(os.path.join(out,\"MLP1.model\"), model1)\n",
    "            serializers.save_npz(os.path.join(out,\"MLP1.state\"), optimizer1)\n",
    "            \n",
    "            #Save training data\n",
    "            pickle.dump(training_loss1,open(os.path.join(out,\"training_loss1.pkl\"),\"wb\"))\n",
    "            pickle.dump(training_accu1,open(os.path.join(out,\"training_accu1.pkl\"),\"wb\"))\n",
    "            \n",
    "            #Save testing data\n",
    "            pickle.dump(testing_loss1,open(os.path.join(out,\"testing_loss1.pkl\"),\"wb\"))\n",
    "            pickle.dump(testing_accu1,open(os.path.join(out,\"testing_accu1.pkl\"),\"wb\"))\n",
    "\n",
    "            time_epoch = time.time() - time_start\n",
    "            log.write('epoch:{:06d} time elapsed:{:0.06f} sec \\r\\n'.format(epoch, time_epoch))\n",
    "            \n",
    "            log.flush()\n",
    "            print('epoch:{:06d} done time elapsed:{:0.06f} sec'.format(epoch, time_epoch))            \n",
    "            \n",
    "            #Reinitialize for next training\n",
    "            sum1_loss = 0\n",
    "            sum1_accu = 0                \n",
    "            train_count = 0\n",
    "            \n",
    "            #Time machine: if something goes wrong, you can always go back\n",
    "            if epoch%save_epoch == 0:\n",
    "                out3 = 'epoch{:06d}'.format(epoch)\n",
    "                if not os.path.isdir(os.path.join(out2, out3)):\n",
    "                    os.mkdir(os.path.join(out2, out3))\n",
    "                    \n",
    "                import shutil\n",
    "                shutil.copy(os.path.join(out,'MLP1.model'),os.path.join(out2,out3))\n",
    "                shutil.copy(os.path.join(out,'MLP1.state'),os.path.join(out2,out3))\n",
    "                shutil.copy(os.path.join(out,'testing_accu1.pkl'),os.path.join(out2,out3))\n",
    "                shutil.copy(os.path.join(out,'testing_loss1.pkl'),os.path.join(out2,out3))\n",
    "                shutil.copy(os.path.join(out,'training_accu1.pkl'),os.path.join(out2,out3))\n",
    "                shutil.copy(os.path.join(out,'training_loss1.pkl'),os.path.join(out2,out3))\n",
    "                shutil.copy(os.path.join(out,'epoch_log.txt'),os.path.join(out2,out3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
